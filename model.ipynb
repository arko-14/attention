{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19gPzTLae5IcTyPcXnJEj1m521SuT8iSY",
      "authorship_tag": "ABX9TyNGVSIHJB4w5eJVEHewfwgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arko-14/attention/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with open(\"/content/drive/MyDrive/wmt14_translate_de-en_train.csv\", 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text_sample = file.read(100000)\n",
        "    print(f\"Successfully loaded {len(text_sample)} characters\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMpSKAHgJm05",
        "outputId": "0e5433f6-3a89-4a45-8e8f-d07bfe410425"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 100000 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = text_sample.lower().split()[:100000]\n",
        "print(f\"Working with {len(words)} words\")\n",
        "\n",
        "\n",
        "word_freq = {}\n",
        "for word in words:\n",
        "    word = word.strip(\".,!?:;\\\"'()[]{}\")\n",
        "    if word:\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "\n",
        "top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:100]\n",
        "print(\"\\nTop 10 words:\")\n",
        "for word, count in top_words[:10]:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFaBbGAJJoLq",
        "outputId": "9da5da3e-1ee6-489d-b00a-431c9f5f485f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with 14877 words\n",
            "\n",
            "Top 10 words:\n",
            "the: 562\n",
            "in: 301\n",
            "die: 297\n",
            "of: 259\n",
            "and: 253\n",
            "der: 252\n",
            "to: 245\n",
            "und: 229\n",
            "a: 164\n",
            "is: 139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words = text_sample.lower().split()[:30000]\n",
        "print(f\"Working with {len(words)} words\")\n",
        "\n",
        "word_freq = {}\n",
        "for word in words:\n",
        "    word = word.strip(\".,!?:;\\\"'()[]{}\")\n",
        "    if word:\n",
        "        word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "\n",
        "top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:100]\n",
        "print(\"\\nTop 10 words:\")\n",
        "for word, count in top_words[:10]:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "H17-rw1_JrBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_file = \"/content/drive/MyDrive/wmt14_translate_de-en_train.csv\"\n",
        "output_file = \"/content/drive/MyDrive/wmt14_30k_sample.csv\"\n",
        "word_limit = 30000\n",
        "word_count = 0\n",
        "collected_sentences = []\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    try:\n",
        "        df = pd.read_csv(input_file)\n",
        "        if 'translation' in df.columns:\n",
        "            text_column = 'translation'\n",
        "        elif 'text' in df.columns:\n",
        "            text_column = 'text'\n",
        "        else:\n",
        "            text_column = df.columns[0]\n",
        "\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            sentence = str(row[text_column])\n",
        "            words_in_sentence = sentence.split()\n",
        "\n",
        "            if word_count + len(words_in_sentence) <= word_limit:\n",
        "                collected_sentences.append(sentence)\n",
        "                word_count += len(words_in_sentence)\n",
        "            else:\n",
        "                words_needed = word_limit - word_count\n",
        "                if words_needed > 0:\n",
        "                    partial_sentence = ' '.join(words_in_sentence[:words_needed])\n",
        "                    collected_sentences.append(partial_sentence)\n",
        "                word_count = word_limit\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CSV reading failed: {e}\")\n",
        "        with open(input_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            for line in file:\n",
        "                words_in_line = line.split()\n",
        "                if word_count + len(words_in_line) <= word_limit:\n",
        "                    collected_sentences.append(line.strip())\n",
        "                    word_count += len(words_in_line)\n",
        "                else:\n",
        "                    words_needed = word_limit - word_count\n",
        "                    if words_needed > 0:\n",
        "                        partial_line = ' '.join(words_in_line[:words_needed])\n",
        "                        collected_sentences.append(partial_line)\n",
        "                    word_count = word_limit\n",
        "                    break\n",
        "\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
        "        for sentence in collected_sentences:\n",
        "            out_file.write(sentence + '\\n')\n",
        "\n",
        "    print(f\"Successfully created a dataset with {word_count} words\")\n",
        "    print(f\"Saved {len(collected_sentences)} sentences to {output_file}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in dataset creation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L241mBDOKRWW",
        "outputId": "d03f3a78-fef6-464e-fba6-873eddcdce63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV reading failed: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n",
            "\n",
            "Successfully created a dataset with 30000 words\n",
            "Saved 684 sentences to /content/drive/MyDrive/wmt14_30k_sample.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/wmt14_30k_sample.csv\", 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Preprocess the text\n",
        "sentences = [line.strip() for line in lines if line.strip()]\n",
        "all_words = []\n",
        "for sentence in sentences:\n",
        "    words = sentence.lower().split()\n",
        "    all_words.extend(words)\n",
        "\n",
        "print(f\"Loaded {len(sentences)} sentences with {len(all_words)} words\")\n",
        "\n",
        "# Build vocabulary\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"Unique words: {len(word_counts)}\")\n",
        "\n",
        "# Create vocabulary mapping\n",
        "vocab = [\"<PAD>\", \"<UNK>\"] + [word for word, _ in word_counts.most_common(10000)]\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "    words = sentence.lower().split()\n",
        "    tokens = [word_to_idx.get(word, 1) for word in words]\n",
        "    tokenized_sentences.append(tokens)\n",
        "\n",
        "# Print some statistics\n",
        "max_len = max(len(tokens) for tokens in tokenized_sentences)\n",
        "avg_len = sum(len(tokens) for tokens in tokenized_sentences) / len(tokenized_sentences)\n",
        "print(f\"Max sentence length: {max_len} tokens\")\n",
        "print(f\"Average sentence length: {avg_len:.2f} tokens\")\n",
        "\n",
        "# Display a few tokenized sentences\n",
        "print(\"\\nSample tokenized sentences:\")\n",
        "for i, tokens in enumerate(tokenized_sentences[:3]):\n",
        "    print(f\"Sentence {i+1}: {tokens}\")\n",
        "\n",
        "\n",
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/tokenized_data_30k.pkl\", 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'tokenized_sentences': tokenized_sentences,\n",
        "        'word_to_idx': word_to_idx,\n",
        "        'idx_to_word': {idx: word for word, idx in word_to_idx.items()}\n",
        "    }, f)\n",
        "\n",
        "print(\"\\nTokenized data saved to 'tokenized_data_30k.pkl'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR823PSxLVfX",
        "outputId": "7c429dce-d7b1-4afe-ccb0-531b6f349e58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 684 sentences with 30000 words\n",
            "Unique words: 10858\n",
            "Max sentence length: 188 tokens\n",
            "Average sentence length: 43.86 tokens\n",
            "\n",
            "Sample tokenized sentences:\n",
            "Sentence 1: [2994]\n",
            "Sentence 2: [2995, 7, 1441, 2996, 437, 37, 3, 2997, 7, 65, 2998, 2999, 7, 29, 3000, 12, 530, 438, 1442, 3001, 2, 677, 3002, 5, 2, 1443, 11, 3003, 3004, 100, 3005, 1444, 8, 3006, 54, 2, 3007, 3008]\n",
            "Sentence 3: [127, 3009, 4, 1445, 31, 274, 531, 9, 4, 439, 31, 1446, 4, 933, 532, 1447, 3010, 3011, 3012, 47, 26, 148, 533, 275, 1448, 166, 16, 1449, 3013, 123, 934, 3, 3014, 8, 534, 1450, 8, 2, 3015, 1451, 3016, 6, 8, 56, 2, 203, 15, 204, 52, 1452, 935, 49, 678, 15, 42, 1453, 187, 1454, 8, 92, 936, 310, 3, 2, 361, 3017]\n",
            "\n",
            "Tokenized data saved to 'tokenized_data_30k.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "max_length = min(100, max(len(tokens) for tokens in tokenized_sentences))\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "# Pad sequences to the same length\n",
        "def pad_sequences(sequences, max_len):\n",
        "    return [seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
        "            for seq in sequences]\n",
        "\n",
        "padded_sequences = pad_sequences(tokenized_sentences, max_length)\n",
        "padded_sequences = np.array(padded_sequences)\n",
        "\n",
        "# Create input and target data for language modeling\n",
        "input_sequences = padded_sequences[:, :-1]\n",
        "target_sequences = padded_sequences[:, 1:]\n",
        "\n",
        "# Convert to TensorFlow dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "dataset = dataset.shuffle(len(tokenized_sentences))\n",
        "dataset = dataset.batch(8)\n",
        "\n",
        "\n",
        "num_layers = 2  # Reduced from 6\n",
        "d_model = 64    # Reduced from 512\n",
        "num_heads = 2   # Reduced from 8\n",
        "dff = 256       # Reduced from 2048\n",
        "\n"
      ],
      "metadata": {
        "id": "uZI2GjofL7bH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"Calculate the attention weights.\"\"\"\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # Add the mask to the scaled tensor\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # Softmax is normalized on the last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                    (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "        return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads) # This line was missing, causing the error\n",
        "        #self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        #self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # Use self.mha here as well\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2 # Return out2, which is the final output of the DecoderLayer\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                             input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                             target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask,\n",
        "             look_ahead_mask, dec_padding_mask): # Remove default values here\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask) # Pass 'training' as keyword argument\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                              self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                          for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        return x\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(…):\n",
        "        super().__init__()\n",
        "        self.self_attn   = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn  = MultiHeadAttention(d_model, num_heads)\n",
        "        # …\n",
        "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
        "        # 1) masked self-attn\n",
        "        attn1, _ = self.self_attn(x, x, x, look_ahead_mask)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn1, training=training))\n",
        "\n",
        "        # 2) encoder-decoder cross-attn\n",
        "        attn2, _ = self.cross_attn(enc_output, enc_output, out1, padding_mask)\n",
        "        out2 = self.layernorm2(out1 + self.dropout2(attn2, training=training))\n",
        "\n",
        "        # 3) FFN\n",
        "        ffn_out = self.ffn(out2)\n",
        "        out3 = self.layernorm3(out2 + self.dropout3(ffn_out, training=training))\n",
        "        return out3\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                          for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,\n",
        "             look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "           x = self.dec_layers[i](x, training=training, mask=look_ahead_mask)\n",
        "\n",
        "            #attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            #attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "def create_model(vocab_size, num_layers=6, d_model=512, num_heads=8,\n",
        "                dff=2048, maximum_position_encoding=10000, rate=0.1):  # Added rate\n",
        "    \"\"\"Create the Transformer model\"\"\"\n",
        "\n",
        "    model = Transformer(\n",
        "        num_layers=num_layers,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        input_vocab_size=vocab_size,\n",
        "        target_vocab_size=vocab_size,\n",
        "        pe_input=maximum_position_encoding,\n",
        "        pe_target=maximum_position_encoding,\n",
        "        rate=rate  # Pass rate to Transformer\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JjyHaCmYMHSQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = create_model(vocab_size, num_layers, d_model, num_heads, dff, max_length)\n",
        "\n",
        "# Define loss and optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "# Define training metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "# Define the training step\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = model(inp, tar, training=True, enc_padding_mask=enc_padding_mask,\n",
        "                               look_ahead_mask=combined_mask, dec_padding_mask=dec_padding_mask)\n",
        "        loss = loss_function(tar, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar, predictions)\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzJaX9udMAsB",
        "outputId": "88df5402-4feb-46c8-886f-52bed1702343"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_22', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'decoder_layer_23', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['transformer_11/encoder_11/embedding_22/embeddings', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_355/kernel', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_355/bias', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_356/kernel', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_356/bias', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_357/kernel', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_357/bias', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_358/kernel', 'transformer_11/encoder_11/encoder_layer_22/multi_head_attention_64/dense_358/bias', 'transformer_11/encoder_11/encoder_layer_22/sequential_44/dense_359/kernel', 'transformer_11/encoder_11/encoder_layer_22/sequential_44/dense_359/bias', 'transformer_11/encoder_11/encoder_layer_22/sequential_44/dense_360/kernel', 'transformer_11/encoder_11/encoder_layer_22/sequential_44/dense_360/bias', 'transformer_11/encoder_11/encoder_layer_22/layer_normalization_110/gamma', 'transformer_11/encoder_11/encoder_layer_22/layer_normalization_110/beta', 'transformer_11/encoder_11/encoder_layer_22/layer_normalization_111/gamma', 'transformer_11/encoder_11/encoder_layer_22/layer_normalization_111/beta', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_361/kernel', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_361/bias', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_362/kernel', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_362/bias', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_363/kernel', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_363/bias', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_364/kernel', 'transformer_11/encoder_11/encoder_layer_23/multi_head_attention_65/dense_364/bias', 'transformer_11/encoder_11/encoder_layer_23/sequential_45/dense_365/kernel', 'transformer_11/encoder_11/encoder_layer_23/sequential_45/dense_365/bias', 'transformer_11/encoder_11/encoder_layer_23/sequential_45/dense_366/kernel', 'transformer_11/encoder_11/encoder_layer_23/sequential_45/dense_366/bias', 'transformer_11/encoder_11/encoder_layer_23/layer_normalization_112/gamma', 'transformer_11/encoder_11/encoder_layer_23/layer_normalization_112/beta', 'transformer_11/encoder_11/encoder_layer_23/layer_normalization_113/gamma', 'transformer_11/encoder_11/encoder_layer_23/layer_normalization_113/beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.2261 Accuracy 0.0000\n",
            "Epoch 1 Batch 10 Loss 9.2197 Accuracy 0.0001\n",
            "Epoch 1 Batch 20 Loss 9.2172 Accuracy 0.0001\n",
            "Epoch 1 Batch 30 Loss 9.2166 Accuracy 0.0001\n",
            "Epoch 1 Batch 40 Loss 9.2147 Accuracy 0.0001\n",
            "Epoch 1 Batch 50 Loss 9.2113 Accuracy 0.0001\n",
            "Epoch 1 Batch 60 Loss 9.2083 Accuracy 0.0002\n",
            "Epoch 1 Batch 70 Loss 9.2047 Accuracy 0.0003\n",
            "Epoch 1 Batch 80 Loss 9.2016 Accuracy 0.0005\n",
            "Epoch 1 Loss 9.2001 Accuracy 0.0006\n",
            "Epoch 2 Batch 0 Loss 9.1424 Accuracy 0.0114\n",
            "Epoch 2 Batch 10 Loss 9.1371 Accuracy 0.0061\n",
            "Epoch 2 Batch 20 Loss 9.1316 Accuracy 0.0063\n",
            "Epoch 2 Batch 30 Loss 9.1239 Accuracy 0.0071\n",
            "Epoch 2 Batch 40 Loss 9.1161 Accuracy 0.0072\n",
            "Epoch 2 Batch 50 Loss 9.1076 Accuracy 0.0076\n",
            "Epoch 2 Batch 60 Loss 9.0980 Accuracy 0.0082\n",
            "Epoch 2 Batch 70 Loss 9.0876 Accuracy 0.0098\n",
            "Epoch 2 Batch 80 Loss 9.0781 Accuracy 0.0115\n",
            "Epoch 2 Loss 9.0734 Accuracy 0.0119\n",
            "Epoch 3 Batch 0 Loss 8.9543 Accuracy 0.0278\n",
            "Epoch 3 Batch 10 Loss 8.9542 Accuracy 0.0217\n",
            "Epoch 3 Batch 20 Loss 8.9413 Accuracy 0.0236\n",
            "Epoch 3 Batch 30 Loss 8.9267 Accuracy 0.0243\n",
            "Epoch 3 Batch 40 Loss 8.9150 Accuracy 0.0242\n",
            "Epoch 3 Batch 50 Loss 8.9026 Accuracy 0.0262\n",
            "Epoch 3 Batch 60 Loss 8.8865 Accuracy 0.0267\n",
            "Epoch 3 Batch 70 Loss 8.8716 Accuracy 0.0268\n",
            "Epoch 3 Batch 80 Loss 8.8553 Accuracy 0.0272\n",
            "Epoch 3 Loss 8.8482 Accuracy 0.0275\n",
            "Epoch 4 Batch 0 Loss 8.7072 Accuracy 0.0530\n",
            "Epoch 4 Batch 10 Loss 8.6495 Accuracy 0.0388\n",
            "Epoch 4 Batch 20 Loss 8.6331 Accuracy 0.0369\n",
            "Epoch 4 Batch 30 Loss 8.6161 Accuracy 0.0353\n",
            "Epoch 4 Batch 40 Loss 8.6070 Accuracy 0.0360\n",
            "Epoch 4 Batch 50 Loss 8.5880 Accuracy 0.0350\n",
            "Epoch 4 Batch 60 Loss 8.5697 Accuracy 0.0359\n",
            "Epoch 4 Batch 70 Loss 8.5521 Accuracy 0.0361\n",
            "Epoch 4 Batch 80 Loss 8.5321 Accuracy 0.0367\n",
            "Epoch 4 Loss 8.5236 Accuracy 0.0366\n",
            "Epoch 5 Batch 0 Loss 8.2909 Accuracy 0.0429\n",
            "Epoch 5 Batch 10 Loss 8.2571 Accuracy 0.0416\n",
            "Epoch 5 Batch 20 Loss 8.2665 Accuracy 0.0415\n",
            "Epoch 5 Batch 30 Loss 8.2213 Accuracy 0.0425\n",
            "Epoch 5 Batch 40 Loss 8.2097 Accuracy 0.0427\n",
            "Epoch 5 Batch 50 Loss 8.1865 Accuracy 0.0428\n",
            "Epoch 5 Batch 60 Loss 8.1669 Accuracy 0.0434\n",
            "Epoch 5 Batch 70 Loss 8.1434 Accuracy 0.0431\n",
            "Epoch 5 Batch 80 Loss 8.1227 Accuracy 0.0431\n",
            "Epoch 5 Loss 8.1162 Accuracy 0.0433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INITIAL_EPOCH = 5  # Start from epoch 5\n",
        "EPOCHS = 10  # Train until epoch 15\n",
        "\n",
        "for epoch in range(INITIAL_EPOCH, EPOCHS +1):\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECBkWs2IPcGV",
        "outputId": "d0c5b93a-ac83-40a7-c381-792caba4009b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Batch 0 Loss 7.8929 Accuracy 0.0316\n",
            "Epoch 6 Batch 10 Loss 7.7945 Accuracy 0.0482\n",
            "Epoch 6 Batch 20 Loss 7.7959 Accuracy 0.0450\n",
            "Epoch 6 Batch 30 Loss 7.7999 Accuracy 0.0421\n",
            "Epoch 6 Batch 40 Loss 7.7855 Accuracy 0.0431\n",
            "Epoch 6 Batch 50 Loss 7.7841 Accuracy 0.0455\n",
            "Epoch 6 Batch 60 Loss 7.7551 Accuracy 0.0467\n",
            "Epoch 6 Batch 70 Loss 7.7356 Accuracy 0.0468\n",
            "Epoch 6 Batch 80 Loss 7.7258 Accuracy 0.0466\n",
            "Epoch 6 Loss 7.7089 Accuracy 0.0472\n",
            "Epoch 7 Batch 0 Loss 7.5351 Accuracy 0.0265\n",
            "Epoch 7 Batch 10 Loss 7.4764 Accuracy 0.0354\n",
            "Epoch 7 Batch 20 Loss 7.4177 Accuracy 0.0346\n",
            "Epoch 7 Batch 30 Loss 7.3872 Accuracy 0.0340\n",
            "Epoch 7 Batch 40 Loss 7.3495 Accuracy 0.0327\n",
            "Epoch 7 Batch 50 Loss 7.3122 Accuracy 0.0326\n",
            "Epoch 7 Batch 60 Loss 7.2809 Accuracy 0.0324\n",
            "Epoch 7 Batch 70 Loss 7.2436 Accuracy 0.0317\n",
            "Epoch 7 Batch 80 Loss 7.2188 Accuracy 0.0319\n",
            "Epoch 7 Loss 7.2068 Accuracy 0.0317\n",
            "Epoch 8 Batch 0 Loss 7.0999 Accuracy 0.0366\n",
            "Epoch 8 Batch 10 Loss 6.8857 Accuracy 0.0351\n",
            "Epoch 8 Batch 20 Loss 6.9095 Accuracy 0.0342\n",
            "Epoch 8 Batch 30 Loss 6.8952 Accuracy 0.0354\n",
            "Epoch 8 Batch 40 Loss 6.8237 Accuracy 0.0382\n",
            "Epoch 8 Batch 50 Loss 6.8011 Accuracy 0.0378\n",
            "Epoch 8 Batch 60 Loss 6.7562 Accuracy 0.0394\n",
            "Epoch 8 Batch 70 Loss 6.7387 Accuracy 0.0417\n",
            "Epoch 8 Batch 80 Loss 6.7101 Accuracy 0.0436\n",
            "Epoch 8 Loss 6.7098 Accuracy 0.0442\n",
            "Epoch 9 Batch 0 Loss 6.7647 Accuracy 0.0644\n",
            "Epoch 9 Batch 10 Loss 6.4135 Accuracy 0.0702\n",
            "Epoch 9 Batch 20 Loss 6.3713 Accuracy 0.0718\n",
            "Epoch 9 Batch 30 Loss 6.3518 Accuracy 0.0743\n",
            "Epoch 9 Batch 40 Loss 6.3344 Accuracy 0.0766\n",
            "Epoch 9 Batch 50 Loss 6.3048 Accuracy 0.0761\n",
            "Epoch 9 Batch 60 Loss 6.2775 Accuracy 0.0794\n",
            "Epoch 9 Batch 70 Loss 6.2462 Accuracy 0.0819\n",
            "Epoch 9 Batch 80 Loss 6.2239 Accuracy 0.0846\n",
            "Epoch 9 Loss 6.2265 Accuracy 0.0842\n",
            "Epoch 10 Batch 0 Loss 4.9335 Accuracy 0.1490\n",
            "Epoch 10 Batch 10 Loss 5.9242 Accuracy 0.1109\n",
            "Epoch 10 Batch 20 Loss 5.8930 Accuracy 0.1092\n",
            "Epoch 10 Batch 30 Loss 5.8434 Accuracy 0.1158\n",
            "Epoch 10 Batch 40 Loss 5.7988 Accuracy 0.1175\n",
            "Epoch 10 Batch 50 Loss 5.8021 Accuracy 0.1165\n",
            "Epoch 10 Batch 60 Loss 5.7755 Accuracy 0.1189\n",
            "Epoch 10 Batch 70 Loss 5.7191 Accuracy 0.1204\n",
            "Epoch 10 Batch 80 Loss 5.6917 Accuracy 0.1200\n",
            "Epoch 10 Loss 5.6779 Accuracy 0.1193\n",
            "Epoch 11 Batch 0 Loss 5.7886 Accuracy 0.1528\n",
            "Epoch 11 Batch 10 Loss 5.4048 Accuracy 0.1460\n",
            "Epoch 11 Batch 20 Loss 5.2917 Accuracy 0.1504\n",
            "Epoch 11 Batch 30 Loss 5.2697 Accuracy 0.1477\n",
            "Epoch 11 Batch 40 Loss 5.2697 Accuracy 0.1461\n",
            "Epoch 11 Batch 50 Loss 5.2698 Accuracy 0.1457\n",
            "Epoch 11 Batch 60 Loss 5.2539 Accuracy 0.1452\n",
            "Epoch 11 Batch 70 Loss 5.2111 Accuracy 0.1462\n",
            "Epoch 11 Batch 80 Loss 5.1824 Accuracy 0.1497\n",
            "Epoch 11 Loss 5.1772 Accuracy 0.1491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INITIAL_EPOCH = 11  # Start from epoch 11\n",
        "EPOCHS = 19  # Train until epoch 20\n",
        "\n",
        "for epoch in range(INITIAL_EPOCH, EPOCHS +1):\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGDWAU7KSMuv",
        "outputId": "31d12a80-7050-471b-c6a9-eeb8148ce292"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Batch 0 Loss 5.1073 Accuracy 0.1894\n",
            "Epoch 12 Batch 10 Loss 4.9211 Accuracy 0.1494\n",
            "Epoch 12 Batch 20 Loss 4.8270 Accuracy 0.1589\n",
            "Epoch 12 Batch 30 Loss 4.7913 Accuracy 0.1638\n",
            "Epoch 12 Batch 40 Loss 4.7983 Accuracy 0.1628\n",
            "Epoch 12 Batch 50 Loss 4.7528 Accuracy 0.1662\n",
            "Epoch 12 Batch 60 Loss 4.7393 Accuracy 0.1702\n",
            "Epoch 12 Batch 70 Loss 4.7395 Accuracy 0.1699\n",
            "Epoch 12 Batch 80 Loss 4.7406 Accuracy 0.1702\n",
            "Epoch 12 Loss 4.7207 Accuracy 0.1709\n",
            "Epoch 13 Batch 0 Loss 4.5191 Accuracy 0.1755\n",
            "Epoch 13 Batch 10 Loss 4.4222 Accuracy 0.1876\n",
            "Epoch 13 Batch 20 Loss 4.4041 Accuracy 0.1846\n",
            "Epoch 13 Batch 30 Loss 4.3746 Accuracy 0.1922\n",
            "Epoch 13 Batch 40 Loss 4.4131 Accuracy 0.1887\n",
            "Epoch 13 Batch 50 Loss 4.3604 Accuracy 0.1888\n",
            "Epoch 13 Batch 60 Loss 4.3461 Accuracy 0.1890\n",
            "Epoch 13 Batch 70 Loss 4.3386 Accuracy 0.1877\n",
            "Epoch 13 Batch 80 Loss 4.3293 Accuracy 0.1876\n",
            "Epoch 13 Loss 4.3466 Accuracy 0.1872\n",
            "Epoch 14 Batch 0 Loss 4.1985 Accuracy 0.2298\n",
            "Epoch 14 Batch 10 Loss 4.1677 Accuracy 0.2017\n",
            "Epoch 14 Batch 20 Loss 4.2097 Accuracy 0.2000\n",
            "Epoch 14 Batch 30 Loss 4.1469 Accuracy 0.2058\n",
            "Epoch 14 Batch 40 Loss 4.0885 Accuracy 0.2123\n",
            "Epoch 14 Batch 50 Loss 4.0936 Accuracy 0.2104\n",
            "Epoch 14 Batch 60 Loss 4.0497 Accuracy 0.2088\n",
            "Epoch 14 Batch 70 Loss 4.0178 Accuracy 0.2058\n",
            "Epoch 14 Batch 80 Loss 4.0140 Accuracy 0.2017\n",
            "Epoch 14 Loss 3.9873 Accuracy 0.2028\n",
            "Epoch 15 Batch 0 Loss 3.8479 Accuracy 0.1932\n",
            "Epoch 15 Batch 10 Loss 3.7887 Accuracy 0.2259\n",
            "Epoch 15 Batch 20 Loss 3.7985 Accuracy 0.2136\n",
            "Epoch 15 Batch 30 Loss 3.7827 Accuracy 0.2118\n",
            "Epoch 15 Batch 40 Loss 3.7369 Accuracy 0.2148\n",
            "Epoch 15 Batch 50 Loss 3.7558 Accuracy 0.2136\n",
            "Epoch 15 Batch 60 Loss 3.6857 Accuracy 0.2187\n",
            "Epoch 15 Batch 70 Loss 3.6817 Accuracy 0.2158\n",
            "Epoch 15 Batch 80 Loss 3.6625 Accuracy 0.2178\n",
            "Epoch 15 Loss 3.6591 Accuracy 0.2169\n",
            "Epoch 16 Batch 0 Loss 3.4662 Accuracy 0.2222\n",
            "Epoch 16 Batch 10 Loss 3.3466 Accuracy 0.2381\n",
            "Epoch 16 Batch 20 Loss 3.3749 Accuracy 0.2341\n",
            "Epoch 16 Batch 30 Loss 3.4220 Accuracy 0.2270\n",
            "Epoch 16 Batch 40 Loss 3.3852 Accuracy 0.2264\n",
            "Epoch 16 Batch 50 Loss 3.3893 Accuracy 0.2272\n",
            "Epoch 16 Batch 60 Loss 3.3772 Accuracy 0.2289\n",
            "Epoch 16 Batch 70 Loss 3.3626 Accuracy 0.2314\n",
            "Epoch 16 Batch 80 Loss 3.3487 Accuracy 0.2330\n",
            "Epoch 16 Loss 3.3483 Accuracy 0.2334\n",
            "Epoch 17 Batch 0 Loss 3.3534 Accuracy 0.2222\n",
            "Epoch 17 Batch 10 Loss 3.1129 Accuracy 0.2345\n",
            "Epoch 17 Batch 20 Loss 3.0495 Accuracy 0.2459\n",
            "Epoch 17 Batch 30 Loss 3.0260 Accuracy 0.2537\n",
            "Epoch 17 Batch 40 Loss 3.0289 Accuracy 0.2504\n",
            "Epoch 17 Batch 50 Loss 3.0055 Accuracy 0.2537\n",
            "Epoch 17 Batch 60 Loss 3.0218 Accuracy 0.2518\n",
            "Epoch 17 Batch 70 Loss 3.0075 Accuracy 0.2535\n",
            "Epoch 17 Batch 80 Loss 3.0294 Accuracy 0.2526\n",
            "Epoch 17 Loss 3.0403 Accuracy 0.2521\n",
            "Epoch 18 Batch 0 Loss 2.6118 Accuracy 0.2109\n",
            "Epoch 18 Batch 10 Loss 2.8895 Accuracy 0.2587\n",
            "Epoch 18 Batch 20 Loss 2.8878 Accuracy 0.2648\n",
            "Epoch 18 Batch 30 Loss 2.8535 Accuracy 0.2728\n",
            "Epoch 18 Batch 40 Loss 2.8444 Accuracy 0.2717\n",
            "Epoch 18 Batch 50 Loss 2.8384 Accuracy 0.2661\n",
            "Epoch 18 Batch 60 Loss 2.7824 Accuracy 0.2699\n",
            "Epoch 18 Batch 70 Loss 2.7737 Accuracy 0.2710\n",
            "Epoch 18 Batch 80 Loss 2.7580 Accuracy 0.2735\n",
            "Epoch 18 Loss 2.7468 Accuracy 0.2729\n",
            "Epoch 19 Batch 0 Loss 2.3444 Accuracy 0.3561\n",
            "Epoch 19 Batch 10 Loss 2.4588 Accuracy 0.2994\n",
            "Epoch 19 Batch 20 Loss 2.4460 Accuracy 0.3029\n",
            "Epoch 19 Batch 30 Loss 2.4912 Accuracy 0.2961\n",
            "Epoch 19 Batch 40 Loss 2.4765 Accuracy 0.2969\n",
            "Epoch 19 Batch 50 Loss 2.4730 Accuracy 0.3012\n",
            "Epoch 19 Batch 60 Loss 2.4551 Accuracy 0.3021\n",
            "Epoch 19 Batch 70 Loss 2.4579 Accuracy 0.2992\n",
            "Epoch 19 Batch 80 Loss 2.4532 Accuracy 0.2969\n",
            "Epoch 19 Loss 2.4494 Accuracy 0.2960\n",
            "Epoch 20 Batch 0 Loss 2.1209 Accuracy 0.3788\n",
            "Epoch 20 Batch 10 Loss 2.1186 Accuracy 0.3230\n",
            "Epoch 20 Batch 20 Loss 2.1109 Accuracy 0.3406\n",
            "Epoch 20 Batch 30 Loss 2.1452 Accuracy 0.3429\n",
            "Epoch 20 Batch 40 Loss 2.1535 Accuracy 0.3402\n",
            "Epoch 20 Batch 50 Loss 2.1669 Accuracy 0.3368\n",
            "Epoch 20 Batch 60 Loss 2.1481 Accuracy 0.3366\n",
            "Epoch 20 Batch 70 Loss 2.1668 Accuracy 0.3296\n",
            "Epoch 20 Batch 80 Loss 2.1589 Accuracy 0.3255\n",
            "Epoch 20 Loss 2.1649 Accuracy 0.3243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INITIAL_EPOCH = 20  # Start from epoch 5\n",
        "EPOCHS = 40  # Train until epoch 15\n",
        "\n",
        "for epoch in range(INITIAL_EPOCH, EPOCHS +1):\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn-yJJeAT_sZ",
        "outputId": "a0b568ed-5a72-41fe-86d6-ed438a9c0667"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 Batch 0 Loss 1.6351 Accuracy 0.5126\n",
            "Epoch 21 Batch 10 Loss 1.8771 Accuracy 0.3527\n",
            "Epoch 21 Batch 20 Loss 1.9104 Accuracy 0.3715\n",
            "Epoch 21 Batch 30 Loss 1.8778 Accuracy 0.3721\n",
            "Epoch 21 Batch 40 Loss 1.8661 Accuracy 0.3712\n",
            "Epoch 21 Batch 50 Loss 1.8645 Accuracy 0.3635\n",
            "Epoch 21 Batch 60 Loss 1.8742 Accuracy 0.3579\n",
            "Epoch 21 Batch 70 Loss 1.8858 Accuracy 0.3565\n",
            "Epoch 21 Batch 80 Loss 1.8977 Accuracy 0.3560\n",
            "Epoch 21 Loss 1.8864 Accuracy 0.3559\n",
            "Epoch 22 Batch 0 Loss 1.5140 Accuracy 0.3775\n",
            "Epoch 22 Batch 10 Loss 1.6284 Accuracy 0.4292\n",
            "Epoch 22 Batch 20 Loss 1.6730 Accuracy 0.3888\n",
            "Epoch 22 Batch 30 Loss 1.6527 Accuracy 0.3857\n",
            "Epoch 22 Batch 40 Loss 1.6599 Accuracy 0.3803\n",
            "Epoch 22 Batch 50 Loss 1.6525 Accuracy 0.3847\n",
            "Epoch 22 Batch 60 Loss 1.6375 Accuracy 0.3822\n",
            "Epoch 22 Batch 70 Loss 1.6347 Accuracy 0.3786\n",
            "Epoch 22 Batch 80 Loss 1.6230 Accuracy 0.3848\n",
            "Epoch 22 Loss 1.6296 Accuracy 0.3832\n",
            "Epoch 23 Batch 0 Loss 1.5544 Accuracy 0.3939\n",
            "Epoch 23 Batch 10 Loss 1.3502 Accuracy 0.3811\n",
            "Epoch 23 Batch 20 Loss 1.3664 Accuracy 0.4072\n",
            "Epoch 23 Batch 30 Loss 1.3603 Accuracy 0.4129\n",
            "Epoch 23 Batch 40 Loss 1.3520 Accuracy 0.4144\n",
            "Epoch 23 Batch 50 Loss 1.3408 Accuracy 0.4141\n",
            "Epoch 23 Batch 60 Loss 1.3588 Accuracy 0.4106\n",
            "Epoch 23 Batch 70 Loss 1.3668 Accuracy 0.4138\n",
            "Epoch 23 Batch 80 Loss 1.3727 Accuracy 0.4058\n",
            "Epoch 23 Loss 1.3629 Accuracy 0.4060\n",
            "Epoch 24 Batch 0 Loss 1.1752 Accuracy 0.2929\n",
            "Epoch 24 Batch 10 Loss 1.1494 Accuracy 0.4114\n",
            "Epoch 24 Batch 20 Loss 1.1674 Accuracy 0.4181\n",
            "Epoch 24 Batch 30 Loss 1.1315 Accuracy 0.4224\n",
            "Epoch 24 Batch 40 Loss 1.1116 Accuracy 0.4278\n",
            "Epoch 24 Batch 50 Loss 1.1233 Accuracy 0.4227\n",
            "Epoch 24 Batch 60 Loss 1.1160 Accuracy 0.4259\n",
            "Epoch 24 Batch 70 Loss 1.1091 Accuracy 0.4195\n",
            "Epoch 24 Batch 80 Loss 1.1207 Accuracy 0.4188\n",
            "Epoch 24 Loss 1.1164 Accuracy 0.4180\n",
            "Epoch 25 Batch 0 Loss 0.9375 Accuracy 0.5240\n",
            "Epoch 25 Batch 10 Loss 0.9314 Accuracy 0.4699\n",
            "Epoch 25 Batch 20 Loss 0.9275 Accuracy 0.4487\n",
            "Epoch 25 Batch 30 Loss 0.9071 Accuracy 0.4354\n",
            "Epoch 25 Batch 40 Loss 0.9082 Accuracy 0.4363\n",
            "Epoch 25 Batch 50 Loss 0.9051 Accuracy 0.4284\n",
            "Epoch 25 Batch 60 Loss 0.9011 Accuracy 0.4271\n",
            "Epoch 25 Batch 70 Loss 0.8981 Accuracy 0.4226\n",
            "Epoch 25 Batch 80 Loss 0.8883 Accuracy 0.4238\n",
            "Epoch 25 Loss 0.8890 Accuracy 0.4239\n",
            "Epoch 26 Batch 0 Loss 0.7398 Accuracy 0.4596\n",
            "Epoch 26 Batch 10 Loss 0.6699 Accuracy 0.4129\n",
            "Epoch 26 Batch 20 Loss 0.6902 Accuracy 0.4120\n",
            "Epoch 26 Batch 30 Loss 0.6656 Accuracy 0.4283\n",
            "Epoch 26 Batch 40 Loss 0.6837 Accuracy 0.4170\n",
            "Epoch 26 Batch 50 Loss 0.6772 Accuracy 0.4221\n",
            "Epoch 26 Batch 60 Loss 0.6715 Accuracy 0.4236\n",
            "Epoch 26 Batch 70 Loss 0.6702 Accuracy 0.4243\n",
            "Epoch 26 Batch 80 Loss 0.6675 Accuracy 0.4254\n",
            "Epoch 26 Loss 0.6693 Accuracy 0.4255\n",
            "Epoch 27 Batch 0 Loss 0.5583 Accuracy 0.4356\n",
            "Epoch 27 Batch 10 Loss 0.4413 Accuracy 0.4503\n",
            "Epoch 27 Batch 20 Loss 0.4739 Accuracy 0.4413\n",
            "Epoch 27 Batch 30 Loss 0.4749 Accuracy 0.4438\n",
            "Epoch 27 Batch 40 Loss 0.4727 Accuracy 0.4372\n",
            "Epoch 27 Batch 50 Loss 0.4795 Accuracy 0.4326\n",
            "Epoch 27 Batch 60 Loss 0.4734 Accuracy 0.4270\n",
            "Epoch 27 Batch 70 Loss 0.4775 Accuracy 0.4314\n",
            "Epoch 27 Batch 80 Loss 0.4787 Accuracy 0.4301\n",
            "Epoch 27 Loss 0.4759 Accuracy 0.4261\n",
            "Epoch 28 Batch 0 Loss 0.3463 Accuracy 0.3295\n",
            "Epoch 28 Batch 10 Loss 0.2889 Accuracy 0.4054\n",
            "Epoch 28 Batch 20 Loss 0.2806 Accuracy 0.4215\n",
            "Epoch 28 Batch 30 Loss 0.2805 Accuracy 0.4239\n",
            "Epoch 28 Batch 40 Loss 0.2813 Accuracy 0.4277\n",
            "Epoch 28 Batch 50 Loss 0.2871 Accuracy 0.4264\n",
            "Epoch 28 Batch 60 Loss 0.2929 Accuracy 0.4247\n",
            "Epoch 28 Batch 70 Loss 0.2986 Accuracy 0.4288\n",
            "Epoch 28 Batch 80 Loss 0.3016 Accuracy 0.4266\n",
            "Epoch 28 Loss 0.3026 Accuracy 0.4262\n",
            "Epoch 29 Batch 0 Loss 0.2813 Accuracy 0.4419\n",
            "Epoch 29 Batch 10 Loss 0.1932 Accuracy 0.4302\n",
            "Epoch 29 Batch 20 Loss 0.1835 Accuracy 0.4344\n",
            "Epoch 29 Batch 30 Loss 0.1778 Accuracy 0.4291\n",
            "Epoch 29 Batch 40 Loss 0.1741 Accuracy 0.4209\n",
            "Epoch 29 Batch 50 Loss 0.1771 Accuracy 0.4286\n",
            "Epoch 29 Batch 60 Loss 0.1797 Accuracy 0.4263\n",
            "Epoch 29 Batch 70 Loss 0.1809 Accuracy 0.4232\n",
            "Epoch 29 Batch 80 Loss 0.1813 Accuracy 0.4272\n",
            "Epoch 29 Loss 0.1813 Accuracy 0.4262\n",
            "Epoch 30 Batch 0 Loss 0.1054 Accuracy 0.4394\n",
            "Epoch 30 Batch 10 Loss 0.0992 Accuracy 0.4334\n",
            "Epoch 30 Batch 20 Loss 0.0995 Accuracy 0.4345\n",
            "Epoch 30 Batch 30 Loss 0.0956 Accuracy 0.4257\n",
            "Epoch 30 Batch 40 Loss 0.0953 Accuracy 0.4350\n",
            "Epoch 30 Batch 50 Loss 0.0950 Accuracy 0.4331\n",
            "Epoch 30 Batch 60 Loss 0.0966 Accuracy 0.4286\n",
            "Epoch 30 Batch 70 Loss 0.0968 Accuracy 0.4272\n",
            "Epoch 30 Batch 80 Loss 0.0967 Accuracy 0.4265\n",
            "Epoch 30 Loss 0.0971 Accuracy 0.4262\n",
            "Epoch 31 Batch 0 Loss 0.0620 Accuracy 0.4318\n",
            "Epoch 31 Batch 10 Loss 0.0536 Accuracy 0.4048\n",
            "Epoch 31 Batch 20 Loss 0.0535 Accuracy 0.4092\n",
            "Epoch 31 Batch 30 Loss 0.0541 Accuracy 0.4095\n",
            "Epoch 31 Batch 40 Loss 0.0522 Accuracy 0.4150\n",
            "Epoch 31 Batch 50 Loss 0.0523 Accuracy 0.4141\n",
            "Epoch 31 Batch 60 Loss 0.0525 Accuracy 0.4186\n",
            "Epoch 31 Batch 70 Loss 0.0519 Accuracy 0.4284\n",
            "Epoch 31 Batch 80 Loss 0.0528 Accuracy 0.4246\n",
            "Epoch 31 Loss 0.0530 Accuracy 0.4262\n",
            "Epoch 32 Batch 0 Loss 0.0260 Accuracy 0.5682\n",
            "Epoch 32 Batch 10 Loss 0.0247 Accuracy 0.4487\n",
            "Epoch 32 Batch 20 Loss 0.0238 Accuracy 0.4550\n",
            "Epoch 32 Batch 30 Loss 0.0246 Accuracy 0.4464\n",
            "Epoch 32 Batch 40 Loss 0.0256 Accuracy 0.4368\n",
            "Epoch 32 Batch 50 Loss 0.0261 Accuracy 0.4285\n",
            "Epoch 32 Batch 60 Loss 0.0266 Accuracy 0.4286\n",
            "Epoch 32 Batch 70 Loss 0.0266 Accuracy 0.4279\n",
            "Epoch 32 Batch 80 Loss 0.0268 Accuracy 0.4278\n",
            "Epoch 32 Loss 0.0269 Accuracy 0.4262\n",
            "Epoch 33 Batch 0 Loss 0.0178 Accuracy 0.3434\n",
            "Epoch 33 Batch 10 Loss 0.0145 Accuracy 0.4511\n",
            "Epoch 33 Batch 20 Loss 0.0137 Accuracy 0.4321\n",
            "Epoch 33 Batch 30 Loss 0.0142 Accuracy 0.4098\n",
            "Epoch 33 Batch 40 Loss 0.0142 Accuracy 0.4153\n",
            "Epoch 33 Batch 50 Loss 0.0146 Accuracy 0.4090\n",
            "Epoch 33 Batch 60 Loss 0.0144 Accuracy 0.4181\n",
            "Epoch 33 Batch 70 Loss 0.0146 Accuracy 0.4206\n",
            "Epoch 33 Batch 80 Loss 0.0148 Accuracy 0.4243\n",
            "Epoch 33 Loss 0.0148 Accuracy 0.4262\n",
            "Epoch 34 Batch 0 Loss 0.0057 Accuracy 0.3333\n",
            "Epoch 34 Batch 10 Loss 0.0070 Accuracy 0.3996\n",
            "Epoch 34 Batch 20 Loss 0.0074 Accuracy 0.3971\n",
            "Epoch 34 Batch 30 Loss 0.0072 Accuracy 0.4124\n",
            "Epoch 34 Batch 40 Loss 0.0076 Accuracy 0.4247\n",
            "Epoch 34 Batch 50 Loss 0.0076 Accuracy 0.4236\n",
            "Epoch 34 Batch 60 Loss 0.0076 Accuracy 0.4329\n",
            "Epoch 34 Batch 70 Loss 0.0076 Accuracy 0.4336\n",
            "Epoch 34 Batch 80 Loss 0.0076 Accuracy 0.4289\n",
            "Epoch 34 Loss 0.0077 Accuracy 0.4262\n",
            "Epoch 35 Batch 0 Loss 0.0039 Accuracy 0.4369\n",
            "Epoch 35 Batch 10 Loss 0.0038 Accuracy 0.4205\n",
            "Epoch 35 Batch 20 Loss 0.0038 Accuracy 0.4311\n",
            "Epoch 35 Batch 30 Loss 0.0038 Accuracy 0.4264\n",
            "Epoch 35 Batch 40 Loss 0.0038 Accuracy 0.4220\n",
            "Epoch 35 Batch 50 Loss 0.0039 Accuracy 0.4229\n",
            "Epoch 35 Batch 60 Loss 0.0040 Accuracy 0.4295\n",
            "Epoch 35 Batch 70 Loss 0.0041 Accuracy 0.4271\n",
            "Epoch 35 Batch 80 Loss 0.0042 Accuracy 0.4271\n",
            "Epoch 35 Loss 0.0042 Accuracy 0.4262\n",
            "Epoch 36 Batch 0 Loss 0.0015 Accuracy 0.5518\n",
            "Epoch 36 Batch 10 Loss 0.0022 Accuracy 0.4151\n",
            "Epoch 36 Batch 20 Loss 0.0022 Accuracy 0.4156\n",
            "Epoch 36 Batch 30 Loss 0.0022 Accuracy 0.4253\n",
            "Epoch 36 Batch 40 Loss 0.0022 Accuracy 0.4248\n",
            "Epoch 36 Batch 50 Loss 0.0022 Accuracy 0.4270\n",
            "Epoch 36 Batch 60 Loss 0.0022 Accuracy 0.4249\n",
            "Epoch 36 Batch 70 Loss 0.0022 Accuracy 0.4271\n",
            "Epoch 36 Batch 80 Loss 0.0023 Accuracy 0.4262\n",
            "Epoch 36 Loss 0.0023 Accuracy 0.4262\n",
            "Epoch 37 Batch 0 Loss 0.0010 Accuracy 0.4962\n",
            "Epoch 37 Batch 10 Loss 0.0012 Accuracy 0.4229\n",
            "Epoch 37 Batch 20 Loss 0.0012 Accuracy 0.4273\n",
            "Epoch 37 Batch 30 Loss 0.0012 Accuracy 0.4196\n",
            "Epoch 37 Batch 40 Loss 0.0012 Accuracy 0.4256\n",
            "Epoch 37 Batch 50 Loss 0.0012 Accuracy 0.4217\n",
            "Epoch 37 Batch 60 Loss 0.0012 Accuracy 0.4148\n",
            "Epoch 37 Batch 70 Loss 0.0012 Accuracy 0.4202\n",
            "Epoch 37 Batch 80 Loss 0.0012 Accuracy 0.4242\n",
            "Epoch 37 Loss 0.0012 Accuracy 0.4262\n",
            "Epoch 38 Batch 0 Loss 0.0017 Accuracy 0.4343\n",
            "Epoch 38 Batch 10 Loss 0.0008 Accuracy 0.4284\n",
            "Epoch 38 Batch 20 Loss 0.0007 Accuracy 0.4305\n",
            "Epoch 38 Batch 30 Loss 0.0007 Accuracy 0.4211\n",
            "Epoch 38 Batch 40 Loss 0.0007 Accuracy 0.4233\n",
            "Epoch 38 Batch 50 Loss 0.0007 Accuracy 0.4277\n",
            "Epoch 38 Batch 60 Loss 0.0007 Accuracy 0.4272\n",
            "Epoch 38 Batch 70 Loss 0.0007 Accuracy 0.4258\n",
            "Epoch 38 Batch 80 Loss 0.0007 Accuracy 0.4269\n",
            "Epoch 38 Loss 0.0007 Accuracy 0.4262\n",
            "Epoch 39 Batch 0 Loss 0.0003 Accuracy 0.4141\n",
            "Epoch 39 Batch 10 Loss 0.0004 Accuracy 0.4147\n",
            "Epoch 39 Batch 20 Loss 0.0004 Accuracy 0.4096\n",
            "Epoch 39 Batch 30 Loss 0.0004 Accuracy 0.4237\n",
            "Epoch 39 Batch 40 Loss 0.0004 Accuracy 0.4234\n",
            "Epoch 39 Batch 50 Loss 0.0004 Accuracy 0.4286\n",
            "Epoch 39 Batch 60 Loss 0.0004 Accuracy 0.4245\n",
            "Epoch 39 Batch 70 Loss 0.0004 Accuracy 0.4233\n",
            "Epoch 39 Batch 80 Loss 0.0004 Accuracy 0.4267\n",
            "Epoch 39 Loss 0.0004 Accuracy 0.4262\n",
            "Epoch 40 Batch 0 Loss 0.0003 Accuracy 0.3889\n",
            "Epoch 40 Batch 10 Loss 0.0002 Accuracy 0.3937\n",
            "Epoch 40 Batch 20 Loss 0.0002 Accuracy 0.4144\n",
            "Epoch 40 Batch 30 Loss 0.0002 Accuracy 0.4066\n",
            "Epoch 40 Batch 40 Loss 0.0002 Accuracy 0.4225\n",
            "Epoch 40 Batch 50 Loss 0.0002 Accuracy 0.4286\n",
            "Epoch 40 Batch 60 Loss 0.0002 Accuracy 0.4278\n",
            "Epoch 40 Batch 70 Loss 0.0002 Accuracy 0.4234\n",
            "Epoch 40 Batch 80 Loss 0.0002 Accuracy 0.4230\n",
            "Epoch 40 Loss 0.0002 Accuracy 0.4262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INITIAL_EPOCH = 40  # Start from epoch 40\n",
        "EPOCHS = 60  # Train until epoch 61\n",
        "\n",
        "for epoch in range(INITIAL_EPOCH, EPOCHS +1):\n",
        "    train_loss.reset_state()\n",
        "    train_accuracy.reset_state()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4bwf2WwX--A",
        "outputId": "49d7fbd1-24e0-45a7-ec67-ac2f42fcab80"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 Batch 0 Loss 0.0001 Accuracy 0.4091\n",
            "Epoch 41 Batch 10 Loss 0.0001 Accuracy 0.3838\n",
            "Epoch 41 Batch 20 Loss 0.0001 Accuracy 0.3783\n",
            "Epoch 41 Batch 30 Loss 0.0001 Accuracy 0.4022\n",
            "Epoch 41 Batch 40 Loss 0.0001 Accuracy 0.4155\n",
            "Epoch 41 Batch 50 Loss 0.0001 Accuracy 0.4170\n",
            "Epoch 41 Batch 60 Loss 0.0001 Accuracy 0.4181\n",
            "Epoch 41 Batch 70 Loss 0.0001 Accuracy 0.4228\n",
            "Epoch 41 Batch 80 Loss 0.0001 Accuracy 0.4245\n",
            "Epoch 41 Loss 0.0001 Accuracy 0.4262\n",
            "Epoch 42 Batch 0 Loss 0.0001 Accuracy 0.4558\n",
            "Epoch 42 Batch 10 Loss 0.0001 Accuracy 0.4622\n",
            "Epoch 42 Batch 20 Loss 0.0001 Accuracy 0.4293\n",
            "Epoch 42 Batch 30 Loss 0.0001 Accuracy 0.4227\n",
            "Epoch 42 Batch 40 Loss 0.0001 Accuracy 0.4275\n",
            "Epoch 42 Batch 50 Loss 0.0001 Accuracy 0.4286\n",
            "Epoch 42 Batch 60 Loss 0.0001 Accuracy 0.4292\n",
            "Epoch 42 Batch 70 Loss 0.0001 Accuracy 0.4323\n",
            "Epoch 42 Batch 80 Loss 0.0001 Accuracy 0.4277\n",
            "Epoch 42 Loss 0.0001 Accuracy 0.4262\n",
            "Epoch 43 Batch 0 Loss 0.0000 Accuracy 0.3611\n",
            "Epoch 43 Batch 10 Loss 0.0000 Accuracy 0.4100\n",
            "Epoch 43 Batch 20 Loss 0.0000 Accuracy 0.4382\n",
            "Epoch 43 Batch 30 Loss 0.0000 Accuracy 0.4407\n",
            "Epoch 43 Batch 40 Loss 0.0000 Accuracy 0.4376\n",
            "Epoch 43 Batch 50 Loss 0.0000 Accuracy 0.4339\n",
            "Epoch 43 Batch 60 Loss 0.0001 Accuracy 0.4293\n",
            "Epoch 43 Batch 70 Loss 0.0001 Accuracy 0.4258\n",
            "Epoch 43 Batch 80 Loss 0.0001 Accuracy 0.4275\n",
            "Epoch 43 Loss 0.0001 Accuracy 0.4262\n",
            "Epoch 44 Batch 0 Loss 0.0000 Accuracy 0.4205\n",
            "Epoch 44 Batch 10 Loss 0.0000 Accuracy 0.4427\n",
            "Epoch 44 Batch 20 Loss 0.0000 Accuracy 0.4268\n",
            "Epoch 44 Batch 30 Loss 0.0000 Accuracy 0.4304\n",
            "Epoch 44 Batch 40 Loss 0.0000 Accuracy 0.4341\n",
            "Epoch 44 Batch 50 Loss 0.0000 Accuracy 0.4289\n",
            "Epoch 44 Batch 60 Loss 0.0000 Accuracy 0.4341\n",
            "Epoch 44 Batch 70 Loss 0.0000 Accuracy 0.4265\n",
            "Epoch 44 Batch 80 Loss 0.0000 Accuracy 0.4264\n",
            "Epoch 44 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 45 Batch 0 Loss 0.0000 Accuracy 0.5909\n",
            "Epoch 45 Batch 10 Loss 0.0000 Accuracy 0.4606\n",
            "Epoch 45 Batch 20 Loss 0.0000 Accuracy 0.4355\n",
            "Epoch 45 Batch 30 Loss 0.0000 Accuracy 0.4327\n",
            "Epoch 45 Batch 40 Loss 0.0000 Accuracy 0.4245\n",
            "Epoch 45 Batch 50 Loss 0.0000 Accuracy 0.4295\n",
            "Epoch 45 Batch 60 Loss 0.0000 Accuracy 0.4250\n",
            "Epoch 45 Batch 70 Loss 0.0000 Accuracy 0.4279\n",
            "Epoch 45 Batch 80 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 45 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 46 Batch 0 Loss 0.0000 Accuracy 0.4937\n",
            "Epoch 46 Batch 10 Loss 0.0000 Accuracy 0.4660\n",
            "Epoch 46 Batch 20 Loss 0.0000 Accuracy 0.4535\n",
            "Epoch 46 Batch 30 Loss 0.0000 Accuracy 0.4464\n",
            "Epoch 46 Batch 40 Loss 0.0000 Accuracy 0.4376\n",
            "Epoch 46 Batch 50 Loss 0.0000 Accuracy 0.4412\n",
            "Epoch 46 Batch 60 Loss 0.0000 Accuracy 0.4306\n",
            "Epoch 46 Batch 70 Loss 0.0000 Accuracy 0.4268\n",
            "Epoch 46 Batch 80 Loss 0.0000 Accuracy 0.4301\n",
            "Epoch 46 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 47 Batch 0 Loss 0.0000 Accuracy 0.5758\n",
            "Epoch 47 Batch 10 Loss 0.0000 Accuracy 0.4093\n",
            "Epoch 47 Batch 20 Loss 0.0000 Accuracy 0.4162\n",
            "Epoch 47 Batch 30 Loss 0.0000 Accuracy 0.4333\n",
            "Epoch 47 Batch 40 Loss 0.0000 Accuracy 0.4407\n",
            "Epoch 47 Batch 50 Loss 0.0000 Accuracy 0.4421\n",
            "Epoch 47 Batch 60 Loss 0.0000 Accuracy 0.4348\n",
            "Epoch 47 Batch 70 Loss 0.0000 Accuracy 0.4249\n",
            "Epoch 47 Batch 80 Loss 0.0000 Accuracy 0.4277\n",
            "Epoch 47 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 48 Batch 0 Loss 0.0000 Accuracy 0.3876\n",
            "Epoch 48 Batch 10 Loss 0.0000 Accuracy 0.4147\n",
            "Epoch 48 Batch 20 Loss 0.0000 Accuracy 0.4226\n",
            "Epoch 48 Batch 30 Loss 0.0000 Accuracy 0.4361\n",
            "Epoch 48 Batch 40 Loss 0.0000 Accuracy 0.4332\n",
            "Epoch 48 Batch 50 Loss 0.0000 Accuracy 0.4338\n",
            "Epoch 48 Batch 60 Loss 0.0000 Accuracy 0.4314\n",
            "Epoch 48 Batch 70 Loss 0.0000 Accuracy 0.4304\n",
            "Epoch 48 Batch 80 Loss 0.0000 Accuracy 0.4296\n",
            "Epoch 48 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 49 Batch 0 Loss 0.0000 Accuracy 0.3864\n",
            "Epoch 49 Batch 10 Loss 0.0000 Accuracy 0.4526\n",
            "Epoch 49 Batch 20 Loss 0.0000 Accuracy 0.4375\n",
            "Epoch 49 Batch 30 Loss 0.0000 Accuracy 0.4227\n",
            "Epoch 49 Batch 40 Loss 0.0000 Accuracy 0.4291\n",
            "Epoch 49 Batch 50 Loss 0.0000 Accuracy 0.4233\n",
            "Epoch 49 Batch 60 Loss 0.0000 Accuracy 0.4259\n",
            "Epoch 49 Batch 70 Loss 0.0000 Accuracy 0.4246\n",
            "Epoch 49 Batch 80 Loss 0.0000 Accuracy 0.4261\n",
            "Epoch 49 Loss 0.0002 Accuracy 0.4262\n",
            "Epoch 50 Batch 0 Loss 0.0000 Accuracy 0.4697\n",
            "Epoch 50 Batch 10 Loss 0.0001 Accuracy 0.4630\n",
            "Epoch 50 Batch 20 Loss 0.0001 Accuracy 0.4607\n",
            "Epoch 50 Batch 30 Loss 0.0008 Accuracy 0.4407\n",
            "Epoch 50 Batch 40 Loss 0.0006 Accuracy 0.4357\n",
            "Epoch 50 Batch 50 Loss 0.0005 Accuracy 0.4338\n",
            "Epoch 50 Batch 60 Loss 0.0006 Accuracy 0.4267\n",
            "Epoch 50 Batch 70 Loss 0.0006 Accuracy 0.4290\n",
            "Epoch 50 Batch 80 Loss 0.0006 Accuracy 0.4290\n",
            "Epoch 50 Loss 0.0006 Accuracy 0.4262\n",
            "Epoch 51 Batch 0 Loss 0.0000 Accuracy 0.3636\n",
            "Epoch 51 Batch 10 Loss 0.0001 Accuracy 0.4340\n",
            "Epoch 51 Batch 20 Loss 0.0001 Accuracy 0.4118\n",
            "Epoch 51 Batch 30 Loss 0.0001 Accuracy 0.4164\n",
            "Epoch 51 Batch 40 Loss 0.0001 Accuracy 0.4230\n",
            "Epoch 51 Batch 50 Loss 0.0001 Accuracy 0.4280\n",
            "Epoch 51 Batch 60 Loss 0.0001 Accuracy 0.4312\n",
            "Epoch 51 Batch 70 Loss 0.0001 Accuracy 0.4270\n",
            "Epoch 51 Batch 80 Loss 0.0001 Accuracy 0.4229\n",
            "Epoch 51 Loss 0.0001 Accuracy 0.4262\n",
            "Epoch 52 Batch 0 Loss 0.0000 Accuracy 0.4116\n",
            "Epoch 52 Batch 10 Loss 0.0000 Accuracy 0.4040\n",
            "Epoch 52 Batch 20 Loss 0.0000 Accuracy 0.4368\n",
            "Epoch 52 Batch 30 Loss 0.0000 Accuracy 0.4354\n",
            "Epoch 52 Batch 40 Loss 0.0001 Accuracy 0.4335\n",
            "Epoch 52 Batch 50 Loss 0.0001 Accuracy 0.4368\n",
            "Epoch 52 Batch 60 Loss 0.0000 Accuracy 0.4260\n",
            "Epoch 52 Batch 70 Loss 0.0000 Accuracy 0.4263\n",
            "Epoch 52 Batch 80 Loss 0.0000 Accuracy 0.4274\n",
            "Epoch 52 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 53 Batch 0 Loss 0.0000 Accuracy 0.2437\n",
            "Epoch 53 Batch 10 Loss 0.0000 Accuracy 0.4176\n",
            "Epoch 53 Batch 20 Loss 0.0000 Accuracy 0.4685\n",
            "Epoch 53 Batch 30 Loss 0.0000 Accuracy 0.4437\n",
            "Epoch 53 Batch 40 Loss 0.0000 Accuracy 0.4391\n",
            "Epoch 53 Batch 50 Loss 0.0000 Accuracy 0.4301\n",
            "Epoch 53 Batch 60 Loss 0.0000 Accuracy 0.4298\n",
            "Epoch 53 Batch 70 Loss 0.0000 Accuracy 0.4276\n",
            "Epoch 53 Batch 80 Loss 0.0000 Accuracy 0.4257\n",
            "Epoch 53 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 54 Batch 0 Loss 0.0000 Accuracy 0.2891\n",
            "Epoch 54 Batch 10 Loss 0.0000 Accuracy 0.4093\n",
            "Epoch 54 Batch 20 Loss 0.0000 Accuracy 0.3989\n",
            "Epoch 54 Batch 30 Loss 0.0000 Accuracy 0.4128\n",
            "Epoch 54 Batch 40 Loss 0.0000 Accuracy 0.4123\n",
            "Epoch 54 Batch 50 Loss 0.0000 Accuracy 0.4166\n",
            "Epoch 54 Batch 60 Loss 0.0000 Accuracy 0.4241\n",
            "Epoch 54 Batch 70 Loss 0.0000 Accuracy 0.4236\n",
            "Epoch 54 Batch 80 Loss 0.0000 Accuracy 0.4232\n",
            "Epoch 54 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 55 Batch 0 Loss 0.0000 Accuracy 0.5770\n",
            "Epoch 55 Batch 10 Loss 0.0000 Accuracy 0.4287\n",
            "Epoch 55 Batch 20 Loss 0.0000 Accuracy 0.4108\n",
            "Epoch 55 Batch 30 Loss 0.0000 Accuracy 0.4076\n",
            "Epoch 55 Batch 40 Loss 0.0000 Accuracy 0.4154\n",
            "Epoch 55 Batch 50 Loss 0.0000 Accuracy 0.4166\n",
            "Epoch 55 Batch 60 Loss 0.0000 Accuracy 0.4251\n",
            "Epoch 55 Batch 70 Loss 0.0000 Accuracy 0.4241\n",
            "Epoch 55 Batch 80 Loss 0.0000 Accuracy 0.4259\n",
            "Epoch 55 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 56 Batch 0 Loss 0.0000 Accuracy 0.4924\n",
            "Epoch 56 Batch 10 Loss 0.0000 Accuracy 0.4478\n",
            "Epoch 56 Batch 20 Loss 0.0000 Accuracy 0.4381\n",
            "Epoch 56 Batch 30 Loss 0.0000 Accuracy 0.4301\n",
            "Epoch 56 Batch 40 Loss 0.0000 Accuracy 0.4234\n",
            "Epoch 56 Batch 50 Loss 0.0000 Accuracy 0.4231\n",
            "Epoch 56 Batch 60 Loss 0.0000 Accuracy 0.4321\n",
            "Epoch 56 Batch 70 Loss 0.0000 Accuracy 0.4287\n",
            "Epoch 56 Batch 80 Loss 0.0000 Accuracy 0.4267\n",
            "Epoch 56 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 57 Batch 0 Loss 0.0000 Accuracy 0.5492\n",
            "Epoch 57 Batch 10 Loss 0.0000 Accuracy 0.4578\n",
            "Epoch 57 Batch 20 Loss 0.0000 Accuracy 0.4666\n",
            "Epoch 57 Batch 30 Loss 0.0000 Accuracy 0.4400\n",
            "Epoch 57 Batch 40 Loss 0.0000 Accuracy 0.4367\n",
            "Epoch 57 Batch 50 Loss 0.0000 Accuracy 0.4336\n",
            "Epoch 57 Batch 60 Loss 0.0000 Accuracy 0.4250\n",
            "Epoch 57 Batch 70 Loss 0.0000 Accuracy 0.4257\n",
            "Epoch 57 Batch 80 Loss 0.0000 Accuracy 0.4259\n",
            "Epoch 57 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 58 Batch 0 Loss 0.0000 Accuracy 0.6364\n",
            "Epoch 58 Batch 10 Loss 0.0000 Accuracy 0.4361\n",
            "Epoch 58 Batch 20 Loss 0.0000 Accuracy 0.4417\n",
            "Epoch 58 Batch 30 Loss 0.0000 Accuracy 0.4396\n",
            "Epoch 58 Batch 40 Loss 0.0000 Accuracy 0.4299\n",
            "Epoch 58 Batch 50 Loss 0.0000 Accuracy 0.4190\n",
            "Epoch 58 Batch 60 Loss 0.0000 Accuracy 0.4255\n",
            "Epoch 58 Batch 70 Loss 0.0000 Accuracy 0.4275\n",
            "Epoch 58 Batch 80 Loss 0.0000 Accuracy 0.4257\n",
            "Epoch 58 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 59 Batch 0 Loss 0.0000 Accuracy 0.4419\n",
            "Epoch 59 Batch 10 Loss 0.0000 Accuracy 0.3974\n",
            "Epoch 59 Batch 20 Loss 0.0000 Accuracy 0.4062\n",
            "Epoch 59 Batch 30 Loss 0.0000 Accuracy 0.4203\n",
            "Epoch 59 Batch 40 Loss 0.0000 Accuracy 0.4202\n",
            "Epoch 59 Batch 50 Loss 0.0000 Accuracy 0.4329\n",
            "Epoch 59 Batch 60 Loss 0.0000 Accuracy 0.4331\n",
            "Epoch 59 Batch 70 Loss 0.0000 Accuracy 0.4263\n",
            "Epoch 59 Batch 80 Loss 0.0000 Accuracy 0.4277\n",
            "Epoch 59 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 60 Batch 0 Loss 0.0000 Accuracy 0.4508\n",
            "Epoch 60 Batch 10 Loss 0.0000 Accuracy 0.4248\n",
            "Epoch 60 Batch 20 Loss 0.0000 Accuracy 0.4352\n",
            "Epoch 60 Batch 30 Loss 0.0000 Accuracy 0.4277\n",
            "Epoch 60 Batch 40 Loss 0.0000 Accuracy 0.4283\n",
            "Epoch 60 Batch 50 Loss 0.0000 Accuracy 0.4234\n",
            "Epoch 60 Batch 60 Loss 0.0000 Accuracy 0.4261\n",
            "Epoch 60 Batch 70 Loss 0.0000 Accuracy 0.4308\n",
            "Epoch 60 Batch 80 Loss 0.0000 Accuracy 0.4244\n",
            "Epoch 60 Loss 0.0000 Accuracy 0.4262\n",
            "Epoch 61 Batch 0 Loss 0.0000 Accuracy 0.4634\n",
            "Epoch 61 Batch 10 Loss 0.0000 Accuracy 0.4332\n",
            "Epoch 61 Batch 20 Loss 0.0000 Accuracy 0.4199\n",
            "Epoch 61 Batch 30 Loss 0.0000 Accuracy 0.4205\n",
            "Epoch 61 Batch 40 Loss 0.0000 Accuracy 0.4175\n",
            "Epoch 61 Batch 50 Loss 0.0000 Accuracy 0.4163\n",
            "Epoch 61 Batch 60 Loss 0.0000 Accuracy 0.4227\n",
            "Epoch 61 Batch 70 Loss 0.0000 Accuracy 0.4228\n",
            "Epoch 61 Batch 80 Loss 0.0000 Accuracy 0.4215\n",
            "Epoch 61 Loss 0.0000 Accuracy 0.4262\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# prompt: code to save the model\n",
        "\n",
        "model.save('my_model.keras') # Or 'my_model.h5'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "F-11B8-5bL96"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "source": [
        "# Assuming 'word_to_idx' is your target vocabulary mapping:\n",
        "idx_to_word_tgt = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "def decode_tokens(tokens):\n",
        "    words = [idx_to_word_tgt.get(token, \"<UNK>\") for token in tokens]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Get the actual token corresponding to 500\n",
        "print(decode_tokens([500]))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1CmxTSNTZOC",
        "outputId": "b29608fc-9ce7-4c76-e3c0-9d836f8cd584"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "network\n"
          ]
        }
      ]
    }
  ]
}